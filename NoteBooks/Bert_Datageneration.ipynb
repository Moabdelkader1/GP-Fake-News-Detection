{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install arabert\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "caQvc0zY9TZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGVx9thT9HF9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "from transformers import AutoTokenizer,AutoModel\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D,Dropout,Conv1D,MaxPooling1D,Reshape\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Gp\\ Dataset"
      ],
      "metadata": {
        "id": "WWoStYAl9XfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeledData=pd.read_csv('labeled_data.csv')\n",
        "labeledData = labeledData.sample(frac=1, random_state=np.random.RandomState(seed=42))\n",
        "#labeledData = labeledData.sample(frac=0.027)\n",
        "labeledData = labeledData.sample(frac=0.1, random_state=np.random.RandomState(seed=42))\n",
        "labeledData = labeledData.reset_index(drop=True)\n",
        "labeledData['label']=labeledData['label'].replace(['not credible','credible'],[0,1])"
      ],
      "metadata": {
        "id": "5O7zDZlW9LUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data=train_test_split(labeledData,test_size=0.2,random_state=42)\n",
        "\n",
        "train_value_counts=train_data['label'].value_counts()\n",
        "val_value_counts=val_data['label'].value_counts()"
      ],
      "metadata": {
        "id": "0rH9NllT9hIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.pie(train_value_counts,labels=train_value_counts.index,autopct='%1.1f%%')\n",
        "plt.title('Pie Chart of Train 0s and 1s')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ArcPNh3Y9ixx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.pie(val_value_counts,labels=val_value_counts.index,autopct='%1.1f%%')\n",
        "plt.title('Pie Chart of Val 0s and 1s')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8ijf_q6s9ko_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
        "ArabertPreprocessor(\n",
        "  model_name= \"\",\n",
        "  keep_emojis = False,\n",
        "  remove_html_markup = True,\n",
        "  replace_urls_emails_mentions = True,\n",
        "  strip_tashkeel = True,\n",
        "  strip_tatweel = True,\n",
        "  insert_white_spaces = True,\n",
        "  remove_non_digit_repetition = True,\n",
        "  replace_slash_with_dash = None,\n",
        "  map_hindi_numbers_to_arabic = None,\n",
        "  apply_farasa_segmentation = None,\n",
        ")\n",
        "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
        "train_arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
        "train_arabert_tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        "train_arabert_model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "val_arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
        "val_arabert_tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        "val_arabert_model = AutoModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "f5ePWp5g9nmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model1(numoffeatures):\n",
        "    model=Sequential()\n",
        "    model.add(Embedding(input_dim=numoffeatures,output_dim=50))\n",
        "    model.add(LSTM(units=64))\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_model2(numoffeatures):\n",
        "    model_lstm = Sequential()\n",
        "    model_lstm.add(Embedding(input_dim=numoffeatures, output_dim=256,))\n",
        "    model_lstm.add(SpatialDropout1D(0.3))\n",
        "    model_lstm.add(LSTM(256, dropout=0.3, recurrent_dropout=0.3))\n",
        "    model_lstm.add(Dense(256, activation='relu'))\n",
        "    model_lstm.add(Dropout(0.3))\n",
        "    model_lstm.add(Dense(1, activation='sigmoid'))\n",
        "    model_lstm.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='Adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    model_lstm.summary()\n",
        "    return model_lstm\n",
        "\n",
        "def create_model3(numoffeatures):\n",
        "    model_lstm=Sequential()\n",
        "    model_lstm.add(Embedding(numoffeatures,100))\n",
        "    model_lstm.add(Dropout(0.2))\n",
        "    model_lstm.add(Conv1D(64, 5, activation='relu'))\n",
        "    model_lstm.add(MaxPooling1D(pool_size=4))\n",
        "    model_lstm.add(LSTM(20, return_sequences=True))\n",
        "    model_lstm.add(LSTM(20))\n",
        "    model_lstm.add(Dropout(0.2))\n",
        "    model_lstm.add(Dense(512))\n",
        "    model_lstm.add(Dropout(0.3))\n",
        "    model_lstm.add(Dense(256))\n",
        "    model_lstm.add(Dense(1, activation='sigmoid'))\n",
        "    model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "    model_lstm.summary()\n",
        "    return model_lstm\n",
        "\n",
        "def create_model4(numoffeatures):\n",
        "    model=Sequential()\n",
        "    model.add(Reshape((64*768,), input_shape=(64, 768)))\n",
        "    model.add(Dense(64, activation='relu',input_dim=numoffeatures))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "fIVLMpIz9oUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(data,preprocesser,tokenizer,model,max_len,batch_size):\n",
        "  num_samples=len(data)\n",
        "  while True:\n",
        "    for offset in range(0, num_samples,batch_size):\n",
        "      batch=data.iloc[offset:offset+batch_size]\n",
        "      batch=batch.reset_index(drop=True)\n",
        "      #print(batch)\n",
        "      embeddings_list=[]\n",
        "      with torch.no_grad():\n",
        "        for i in range(len(batch)):\n",
        "          text = batch.loc[i,'title']\n",
        "          preprocessed=preprocesser.preprocess(text)\n",
        "          tokenized=' '.join(tokenizer.tokenize(preprocessed))\n",
        "          input_ids = tokenizer.encode(tokenized, padding=\"max_length\", truncation=True, max_length=max_len, return_tensors='pt')\n",
        "          embeddings=model(input_ids)[0]\n",
        "          embeddings_list.append(embeddings.detach().numpy())\n",
        "      x=np.concatenate(embeddings_list, axis=0)\n",
        "      y=batch['label']\n",
        "      y=y.values\n",
        "      print(y.shape)\n",
        "      print(x.shape)\n",
        "      yield x,y"
      ],
      "metadata": {
        "id": "Y7xAlWh19tAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen=data_generator(train_data,train_arabert_prep,train_arabert_tokenizer,train_arabert_model,64,64)\n",
        "val_gen=data_generator(val_data,val_arabert_prep,val_arabert_tokenizer,val_arabert_model,64,64)"
      ],
      "metadata": {
        "id": "lveuNo-59unu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_steps_per_epoch = len(train_data)//batch_size\n",
        "if len(train_data) % batch_size != 0:\n",
        "    train_steps_per_epoch += 1\n",
        "print(train_steps_per_epoch)\n",
        "\n",
        "val_steps_per_epoch = len(val_data)//batch_size\n",
        "if len(val_data) % batch_size != 0:\n",
        "    val_steps_per_epoch += 1\n",
        "print(val_steps_per_epoch)"
      ],
      "metadata": {
        "id": "xTLC0PkV9xRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=create_model4(768)\n",
        "#X_train,X_test,Y_train,Y_test=train_test_split(embeddings_array,y,test_size=0.2,random_state=20)\n",
        "checkpoint_callback = ModelCheckpoint('model_weights_{epoch:02d}.h5', save_weights_only=True)\n",
        "model.fit(train_gen,steps_per_epoch=train_steps_per_epoch,validation_data=val_gen,validation_steps=val_steps_per_epoch, epochs=5,verbose=1,callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "0Te_AAyj9zgH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}